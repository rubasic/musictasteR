---
title: "MusictasteR"
author: "Roberta Conrad, Clara Dionet, Mirae Kim, Jorgen Lund, Akshay Sundar"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{dplyr compatibility}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This vignette describes the MusicTasteR package, the process from its conception to its execution, and its final capabilities and use cases.

#
#Table of Contents
###Overview
- Data
- The App
- Functions

###Data Grabbing
- Top Songs
- Average Songs

###Package functions
- Search
- Compare: Hover Plot
- Rewind: Attributes Over Time
- Predict: Logistic Regression
- Match: Clustering

# 
#Overview

The package musictasteR was designed to compare and contrast the music features of top and average songs. We wanted to answer questions such as: what are the characteristics of popular songs? Are we able to predict the popularity of a given song? What songs are similar to each other in terms of their attrbiutes? 

##Data
To respond to this problem, we assembled more than 50k top songs (songs which have been featured in the Top100 Billboard chart) and 150k average songs over the years 1960 to 2015. How the data was collected is explained in detail in the "Data Grabbing" section.

##The App

The main functionality of this package is the shiny application, which allows to visualise, compare and cluster top and average songs according to their music characteristics. The user can access it by running the following command:

```{r eval=F}
launch.shiny()
```

The app contains a sidebar on the left which allows the user to search any song, add it to his save tracks and compare it with other all others songs. The search function takes as input any string and returns a list of the songs most related to this string. The user simply ticks the song(s) he wishes to add to his saved tracks.

The first tab, "Compare" plots billboard songs according to 2 attributes entered by the user for a given year. The user may also add any input song(s) for comparison and they will be displayed in pink.

The second tab, "Rewind" plots the music characteristics of top and/or average (Billboard vs. Non Billboard) songs over time. The user can specify the attributes, the time range, type of popularity and type of plot to be displayed. The boxplots gives more information about the data as it shows the distribution of each variable per year, whereas the average plots allow a global visualisation of multiple attributes. It is recommended to plot only up to 2-3 features for boxplot as the plot rapidly gets packed.

The third tab, "Predict" plots the probabilities for the user's saved tracks to be in the top 100. The minimum and maximum probabilities, along with the release year are labeled.

The fourth tab, "Match" plots the clusters of the billboard songs according to the first two principal components. The user may also add any input song(s) for comparison. Songs clustered together (of the same color) are slightly similar in terms of musical features, whereas songs closeby in the plot are strongly similar.

##Functions

Our package contains a set functions created to build our shiny application which can be run directly by the user. All functions (motivations, input arguments, use cases) are described in detail in the "Package Functions" section.

The user needs to first go on spotify to get their Client ID:
https://developer.spotify.com/documentation/web-api/

Prior to running the Shiny App or using the functions in the musictasteR package the user must set his/her client credentials in the following way on the console -

```{r eval=FALSE}
Sys.setenv(SPOTIFY_CLIENT_ID = 'client id')
Sys.setenv(SPOTIFY_CLIENT_SECRET = 'client secret')
```

The app will then generate an access token to query the Spotify API with.


#Data Gathering

We needed a sample of average and top chart songs that we could analyse and see if there were different characteristics between the two song types. We explored various sources of music data on the net, including MusicBrainz (https://musicbrainz.org), Discogs api (https://www.discogs.com/developers/), Echo Nest (http://the.echonest.com/) but eventually settled on Spotify due to their lenient API rate usage policies, and the simplicity of the music attribute outputs. 

## Top Songs
For the top charts data, we found the following package: https://cran.r-project.org/web/packages/billboard/index.html, which gave us a dataset of about 90-100 top charts songs froom 1960 to 2015. It already had the spotify uri and the audio features included in the dataframe, which was very convenient for us to use.

## Average Songs

In order to build a database of "average" songs, we searched on the internet for a flat file of a sample of average songs. We came across the One Million Song Database (https://labrosa.ee.columbia.edu/millionsong/) which we tried to use as our sample of "average songs." However, we ran into two problems with this dataset. The first was that using track name and artist name to search for the spotify URL did not always yield accurate matches, as song titles and artist names could be different across sources. Another problem was that we only had data from the 1980s onwards with most of the data coming from the 2010s, which was not the distribution of the billboard data we had. Therefore, we decided to supplement the years we were missing with data sampled directly from spotify.

Spotify api search track function that had a wildcard feature, and it also provided a filter on the album year. We decided to leverage these features to sample random songs directly from spotify. 

Below is the function that we used to match the songs from the one million song database to the spotify data.

```{r}
library(gridExtra)
library(dplyr)
library(musictasteR)
library(billboard)
library(reshape)
```

```{r eval=F}
get_track_id <- function(row) {
  track_name = gsub(' ','%20', gsub("[^[:alnum:][:space:]]",'',row['song']))
  artist_name =gsub(' ','%20', gsub("[^[:alnum:][:space:]]",'',row['artist']))
  api <- str_glue('https://api.spotify.com/v1/search/?q=track:{track_name}%20artist:{artist_name}&type=track&limit=1')
  access_token <- get_spotify_access_token()
  result <- RETRY('GET', url = api, query = list(access_token = <INSERT_ACCESS_TOKEN>), quiet = TRUE, times = 1, pause_min = 1) %>% content 
    tryCatch({
      final_id <- suppressWarnings(result$tracks$items[[1]]$id)
      return (final_id)
    }, error=function(error_cond) {
      if (error_cond$message == 'subscript out of bounds') {
        return('ERROR subscript')
      }
    }
    , error=function(error_cond) {
      if (error_cond$message =='$ operator is invalid for atomic vectors') {
        result2 <- RETRY('GET', url = api, query = list(access_token = <INSERT_ACCESS_TOKEN>), quiet = TRUE, times = 1, pause_min = 1) %>% content 
        print(error_cond$message)
        print(api)
        return(result2$tracks$items[[1]]$id)
      }
      else {
        print(error_cond$message)
        return('ERROR mysterious')
      }
    }
  )
}
```

###Sampling the Data:

Script that searches by year based on top words in song titles -

The Spotify API was queried with an input in the search bar and an input year. The search bar input was looped on a list of all the top 99 words in song titles. The total list of responses were recorded for processing.

```{r eval=F}
all_tracks_upd <- list()
for (i in 1:98) {
  for (year in 1960:2019) {
    
    # Top words is a list of top 99 words appearing frequently in song title, link given in the description
    track_search <-  paste0('*',top_words[i,],'*')
    
    #URL to query
    url_built <- str_glue('https://api.spotify.com/v1/search?q=track:{track_search}%20year:{year}&limit=50&offset=60&type=track')
      res <- RETRY('GET', url = url_built, query = list(access_token = access_token), quiet = TRUE, times = 1, pause_min = 100) %>% content
    if((!is.null(names(res)))){
    res <- res %>% .$tracks %>% .$items
    all_tracks_upd <- append(all_tracks_upd,res)
    }
  }
  # Testing to see if the word was correctly used
  print(top_words[i,])
}

```

```{r eval=F}
#transforms list of results into a dataframe
tracks_df_upd <- map_df(seq_len(length(all_tracks_upd)), function(x) {
                list(
                    track_name = all_tracks_upd[[x]]$name,
                    track_uri = gsub('spotify:track:', '', all_tracks_upd[[x]]$uri),
                    artist_name = all_tracks_upd[[x]]$artists[[1]]$name,
                    artist_uri = all_tracks_upd[[x]]$artists[[1]]$id,
                    album_name = all_tracks_upd[[x]]$album$name,
                    album_id = all_tracks_upd[[x]]$album$id,
                    album_year= all_tracks_upd[[x]]$album$release_date
                )
            })

# Pulling the last 4 digits in the year, because some songs contain the full date of release
sample_by_year_upd <- tracks_df_upd %>% mutate(album_year_4dgt=substr(album_year,1,4) ) 
```

Test to see if restriction at 7 songs per artist per year will cut down the total songs by a considerable amount -
```{r eval=F}
artist_freq <- sample_by_year_upd %>%
  group_by(artist_name,album_year_4dgt) %>%
  summarise(freq=n()) 

artist_freq$recalc_songs=ifelse(artist_freq$freq>7,7,artist_freq$freq)

#Total songs
artist_freq %>%
  ungroup() %>%
  summarise(tots=sum(recalc_songs))


ggplot(data=artist_freq, aes(x=freq)) + geom_histogram(bins=30) 
```


The list of tracks was restricted to 7 songs per artist per year, to prevent over-sampling from one artist. Selection of songs was a random process.

```{r eval=F}
sample_songs_edit <- sampled_songs_worded %>%
  select(-album_year) %>%
  distinct() %>%
  arrange(album_year_4dgt,artist_name) %>%
  group_by(album_year_4dgt,artist_name) %>%
  slice(1:7) %>%
  ungroup()

# Test to see distribution
sample_songs_edit %>% distinct(artist_name) %>% summarise(total_artists=n())

sample_songs_edit %>% group_by(album_year_4dgt) %>% summarise(count=n()) %>% ggplot(aes(x=album_year_4dgt,y=count)) + geom_bar(stat='identity')
```

A Max cap of 3000 songs was applied per year, to prevent excess processing.

```{r eval=F}
sample_songs_edit <- sample_songs_edit %>%
  arrange(album_year_4dgt,artist_name)

#First keep as many unique artists as possible
sample_songs_edit$count=1

# Setting a counter for each song by an artist in a year
for(i in 2:nrow(sample_songs_edit)){
  if(sample_songs_edit$artist_name[i]==sample_songs_edit$artist_name[i-1]&sample_songs_edit$album_year_4dgt[i]==sample_songs_edit$album_year_4dgt[i-1]){
    sample_songs_edit$count[i]=sample_songs_edit$count[i-1]+1
  }
}

# Selecting on the basis of the counter variable, so that 1 song from each artist is selected and then 2nd from each artist and so on
sample_songs_edit <- sample_songs_edit %>%
  arrange(album_year_4dgt,artist_name,count) %>%
  group_by(album_year_4dgt) %>%
  slice(1:3000) %>%
  ungroup()

sample_songs_edit %>% distinct(artist_name) %>% summarise(total_artists=n())

sample_songs_edit %>% group_by(album_year_4dgt) %>% summarise(count=n()) %>% ggplot(aes(x=album_year_4dgt,y=count)) + geom_bar(stat='identity')
```

Sample_songs_edit is the final pre-processed dataframe used.


The following functions were used to grab the audio features from spotify:
```{r eval=F}
make_api_call <- function(audio_features_api,access_token) {
  analysis_result_call <- safely(RETRY)('GET', url = audio_features_api, query = list(access_token = access_token), quiet = TRUE, times = 1, pause_min = 100) 
  if (is.null(analysis_result_call$result)) {
    print("ERROR in call for audio features")
    print(analysis_result_call$error)
  } else {
  analysis_result <- analysis_result_call$result %>% content
  track_href <- analysis_result$track_href
  track_result_call <- safely(RETRY)('GET', url = track_href, query = list(access_token = access_token), quiet = TRUE, times = 1, pause_min = 100)
  analysis_result <- analysis_result[c('danceability',
                                       'energy',
                                       'key',
                                       'loudness',
                                       'mode',
                                       'speechiness',
                                       'acousticness',
                                       'instrumentalness',
                                       'liveness',
                                       'valence',
                                       'tempo',
                                       'type',
                                       'uri',
                                       'track_href',
                                       'analysis_url',
                                       'duration_ms',
                                       'time_signature')]
    track_basic_trait_result <- track_result_call$result %>% content
    analysis_result['artist_name'] <- track_basic_trait_result$artists[[1]]$name
    analysis_result['track_name'] <- track_basic_trait_result$name
    analysis_result['year'] <- track_basic_trait_result$album$release_date
  return(analysis_result)
  }
}

get_audio_features <- function(row) {
    track_uri <- as.character(row["track_uri"])
    audio_features_api <- str_glue('https://api.spotify.com/v1/audio-features/{track_uri}')
    access_token <- get_spotify_access_token()
    analysis_result <- safely(make_api_call)(audio_features_api,access_token)
    if (is.null(analysis_result$error)) {
      return(analysis_result)
    } else {
      print("ERROR in overall call")
      print(analysis_result$error)
    }
}


head_of_df <- df
final_list <- list()
fail_list <- list()

#loop that runs the api fetch of musi
for (i in 1:nrow(head_of_df)) {
  if (i %% 20 == 0) {
    print(i)
    #randomly sleep for a little bit   
    Sys.sleep(sample(seq(0.5, 2.5, 0.5), 1))
  }
  row <- unlist(head_of_df[i,])
  api_result <- get_audio_features(row)
  #error hndling
  if (is.null(api_result$result)) {
    #wrong url
    if (api_result$message == "Must specify at least one of url or handle") {
      print(paste("Error in id", i))
      # return("URL misinput error")
    } else {
      print(paste('Other kind of error in id', i))
      print(api_result$message)
    }
    ## put other error handling here
  } else {
    final_row <- c(unlist(row[c("track_name", "track_uri", "album_name", "album_year", "album_year_4dgt", "artist_name", "artist_uri")]), unlist(api_result$result))
    final_list[[i]] <- final_row
  }
}

#filter out the null entries in the list
final_list_filtered <- Filter(Negate(is.null), final_list)

#make the list into a dataframe
massive <- map_df(seq_len(length(final_list_filtered)), function(x) {
                list(
                  track_name = final_list_filtered[[x]]["track_name"],
                  track_uri = final_list_filtered[[x]]["track_uri"],
                  album_name = final_list_filtered[[x]]["album_name"],
                  album_year = final_list_filtered[[x]]["album_year"],
                  album_year_4dgt = final_list_filtered[[x]]["album_year_4dgt"],
                  danceability = final_list_filtered[[x]]["danceability"],
                  energy = final_list_filtered[[x]]["energy"],
                  key = final_list_filtered[[x]]["key"],
                  loudness = final_list_filtered[[x]]["loudness"],
                  mode = final_list_filtered[[x]]["mode"],
                  speechiness = final_list_filtered[[x]]["speechiness"],
                  acousticness = final_list_filtered[[x]]["acousticness"],
                  instrumentalness = final_list_filtered[[x]]["instrumentalness"],
                  liveness = final_list_filtered[[x]]["liveness"],
                  valence = final_list_filtered[[x]]["valence"],
                  tempo = final_list_filtered[[x]]["tempo"],
                  type = final_list_filtered[[x]]["type"],
                  uri = final_list_filtered[[x]]["uri"],
                  track_href = final_list_filtered[[x]]["track_href"],
                  analysis_url = final_list_filtered[[x]]["analysis_url"],
                  duration_ms = final_list_filtered[[x]]["duration_ms"],
                  time_signature = final_list_filtered[[x]]["time_signature"],
                  artist_name = final_list_filtered[[x]]["artist_name"],
                  artist_id = final_list_filtered[[x]]["artist_uri"]
                )})
```

#
# Package Functions

### Search

The get_tracks_artists function takes a string and returns a data frame with track information for the 20 best matches from Spotify's search endpoint. It was created to allow the user of the musictasteR app to search for tracks and/or artists. The input is the string that is used in the search, and a Spotify access token that allows access to the Spotify API.

Example: if the user inputs the string "Thriller", the function will return a data frame with information about the 20 best matches for this string in the Spotify database.

### Compare: Hover Plot

To [Roberta] 

Description of function, use cases (what are the inputs/outputs or examples)

### Rewind: Attributes Over Time

#### Description

In order to visualize and compare the evolution of top and average song attributes (such as "valence", "danceability", etc.), we decided to implement the attributes_time function. 

This function plots overlapping variables (chosen by the user) of 2 different data sets as a function of the "year" column. In our case these two dataframes correspond to top and average songs ("Billboard" and "Non Billboard" respectively).

The song attributes can either be layed out as averages or as boxplots. The boxplot gives us more information about the data as it shows the distribution of each variable per year. Plotting averages on the other hand allows us to have a global overview of all the variables at the same time (boxplot plots only up to 2-3 variables clearly).

The choice to take as arguments 2 dataframes instead of 1 (with an additional column specifying the category of each dataset) was to avoid unnecessary formatting in case of very different datasets. If the user only needs to plot one overlapping feature in his 2 datasets, he does not need to match every column name to each other before applying the rbind function in order to store them as one dataframe. 

This was appropriate in our case as the billboard data was collected from a different source than that of average songs, but both contained the same music features from the Spotify API.

#### Inputs
The input arguments are as follows:

First dataset characteristics:
- df1: a data.frame 

- df1_title: title of your first df (ex: topsongs) 

- df1_year_col: a number (column number of the "year" column)  


Second dataset characteristics: 

- df2: a data.frame  

- df2_title: title of your second df (ex: averagesongs) 

- df2_year_col: a number (column number of the "year" column) 


Plot characteristics: 
- attributes: a vector containing strings, the column names to be plotted (ex: c("energy","danceability")) 

- boxplot: a boolean, specifies the type of plot (TRUE = plot as boxplots, FALSE = plot as averages) 

- timerange: a 2x1 vector, this defines xlims, years to be plotted over (ex: c(1990, 2010)) 

- title_vector: a string vector with the df_title names (the strings have to match!). This defines what will be plotted, for example, c(df1_title) will plot only the first dataframe, and c(df1_title, df2_title) will plot both. Although this is a redundant input parameter, it was created to facilitate the synchronization with the shiny app (the user can choose from the "Billboard" or "Non Billboard" checkboxes).

#### Examples: 
Load data & define inputs:

```{r}
data("averagesongs")
topsongs <- billboard::spotify_track_data
colnames(averagesongs)
colnames(topsongs)
```
The variable "year" is in column 1 for topsongs and column 4 for average songs.

Comparing averages for Billboard / Non Billboard data:
```{r fig.width=7}
attributes <- c("danceability" ,"energy" ,"mode", "speechiness","acousticness", "instrumentalness" ,"liveness","valence" ,"tempo")
boxplot <- FALSE
timerange <- c(1960,2015)
title_vector <- c("Top Songs","Average Songs") # plot both

attributes_time(topsongs, "Top Songs", 1, averagesongs, "Average Songs", 4, attributes, boxplot, timerange, title_vector)
```

Plot only billboard data:
```{r fig.width=7}
title_vector <- "Average Songs"
attributes_time(topsongs, "Top Songs", 1, averagesongs, "Average Songs", 4, attributes, boxplot, timerange, title_vector)
```

Plot as boxplots:
```{r fig.width=7}
boxplot <- TRUE
title_vector <- c("Billboard","Non Billboard")
attributes <- c("danceability")
# compare billboard and non billboard for "danceability""
attributes_time(topsongs, "Billboard", 1, averagesongs, "Non Billboard", 4, attributes, TRUE, c(1960,2015), title_vector)

# compare "danceability" and "energy" for billboard data only
attributes <- c("danceability","energy")
title_vector <- c("Billboard")
attributes_time(topsongs, "Billboard", 1, averagesongs, "Non Billboard", 4, c("danceability","energy"), TRUE, c(1960,2015), title_vector)

```


### Predict: Logistic Regression

#### Description

One of the features of the app is to predict how successful a song would perform. For this, we implemented a logistic regression to find the probability of whether a song would be in the top charts or not. First, we needed to make sure that for every year, we had an equal proportion of non-billboard songs to billboard songs. Because the billboard songs quantity was relatively fixed across the years, we reduced the number of average songs in our sample per year. Variable selection for the logistic regression was performed using the stepwise method that used both the forward and backward procedure optimizing on the BIC criterion. We calibrated one model per year and saved it into a list of models, along with their fisher test scores to check global signficance of the model. The models were saved as a .rda object and is accessible in the package as "log_model_list."

```{r eval=F}
#load the large data
df <- averagesongs
df <- df[!is.na(df$key),]
billboard_df <- billboard::spotify_track_data

#prepare large dataframes, align columns and combine the datasets
#this is the average songs data
df_edit <- df[c("track_uri","year","track_name", "artist_name","duration_ms", "danceability","energy","key","loudness","mode","speechiness","acousticness","instrumentalness","liveness","valence","tempo")]
colnames(df_edit)[1] <- "track_id"
df_edit["top_or_not"] <- 0

#this is the billboard data
billboard_edit <- billboard_df[c("track_id","year","track_name", "artist_name","duration_ms", "danceability","energy","key","loudness","mode","speechiness","acousticness","instrumentalness","liveness","valence","tempo")]
billboard_edit["top_or_not"] <- 1
full_test <- rbind(billboard_edit,df_edit)
full_test <- full_test[!(duplicated(full_test$track_name)& duplicated(full_test$artist_name)),]

#filter out artists that show up more than 3 times
artist_test <- full_test %>% filter(top_or_not == 0) %>% group_by(year, artist_name) %>% slice(1:3)
artist_test%>% group_by(year, artist_name) %>% summarise(count=n())
artist_test <- artist_test %>% group_by(year) %>% slice(1:600) %>% ungroup() #limit each year to only have 600 average songs
artist_test %>% group_by(year) %>% summarise(count=n()) 
new_artist_test <-rbind(billboard_edit, artist_test) #finally, combine the average and the billboard songs
per_year_test <- new_artist_test %>% group_by(year) %>% slice(1:600)
```

The following code shows the analysis we did to tailor the number of average and billboard songs per year before running the logistic regression ont he data.
```{r fig.width=8, eval=F}
#Original Distribution of songs per year
full_test %>% filter(!year%in% c(2015:2019)) %>% group_by(year, top_or_not) %>% summarise(count=n()) %>% ggplot(aes(x=year, y=count, fill=as.factor(top_or_not))) + geom_bar(stat="identity") + labs(title="Original Distribution of Average to Top Chart Songs")

#Look at the distribution of songs per year
new_artist_test %>% filter((!year%in% c(2015:2019))) %>% group_by(year, top_or_not) %>% summarise(count=n()) %>% ggplot(aes(x=year, y=count, fill=as.factor(top_or_not))) + geom_bar(stat="identity") + labs(title="Filtered Distribution of Average to Top Chart Songs")

#define function that runs logistic regression for one year
run_l_reg <-function(full_test) {
  res <- glm(top_or_not~.,family=binomial(link='logit'), data=full_test[-c(1:4)])
  bic <- step(res,direction="both",k=3,trace = FALSE)
  res2 <- glm(bic$model,family=binomial(link='logit'),data=full_test[-c(1:4)])
  anova_res <- anova(update(res2, ~1), res2, test='LR')
  return(list(bic, anova_res$`Pr(>Chi)`[2]))
}

#run the logistic regression for every year
get_all_reg_models <- function(full_test) {
  all_results <- list()
  anova_list <- list()
  return_list <- list()
  for (year_input in 1960:2015) {
    full_test_year <- full_test %>% filter(year == year_input)
    result <- run_l_reg(full_test_year)
    all_results[paste0('year', as.character(year_input))] <- result[1]
    anova_list[paste0('year', as.character(year_input))] <- result[2]
  }
  return_list[['model_list']] <- all_results
  return_list[['anova_list']] <- anova_list
  return(return_list)
}

clean_up <- function(input) {
                list(
                  danceability = input$result$danceability,
                  energy = input$result$energy,
                  key = input$result$key,
                  loudness = input$result$loudness,
                  mode = input$result$mode,
                  speechiness = input$result$speechiness,
                  acousticness = input$result$acousticness,
                  instrumentalness = input$result$instrumentalness,
                  liveness = input$result$liveness,
                  valence = input$result$valence,
                  tempo = input$result$tempo,
                  duration_ms =  input$result$duration_ms,
                  # uri = final_list_filtered[[x]]["uri"],
                  # track_href = final_list_filtered[[x]]["track_href"],
                  # analysis_url = final_list_filtered[[x]]["analysis_url"],
                  # duration_ms = final_list_filtered[[x]]["duration_ms"],
                  # time_signature = final_list_filtered[[x]]["time_signature"],
                  artist_name = input$result$artist_name,
                  track_name = input$result$track_name,
                  year = input$result$year
                  # artist_id = final_list_filtered[[x]]["artist_uri"]
                )
  }

get_all_reg_models(new_artist_test)

save_result <- get_all_reg_models(new_artist_test)

# saveRDS(save_result$model_list, file="all_models2.rda")

```

All models had significant liklihood ratio test scores. 1972 has the highest Liklihood ratio test score of 2.86441e-09, meaning it is the least significant model. The models are saved in the package and can be accessed through an object called "log_model_list"
```{r eval=F}
anova_list <- unlist(save_result$anova_list)
anova_df <- data.frame(anova_score = unlist(anova_list))
anova_df['year'] <- sapply(rownames(anova_df), function(x) {return(substring(x,5,9))})

# All anova scores
a <- ggplot() + geom_point(data=anova_df,aes(x=year, y=anova_score))+labs(main="Anova score for models")
b <- anova_df %>% filter(year != 1972) %>% ggplot()+ geom_point(aes(x=year, y=anova_score))+labs(main="Anova scores for models except 1972")
grid.arrange(a,b, ncol=2)
```

Below is a sample output of one of the models (1960 model) in the model list object. For every year, we can see the variables that were relevant in the prediction of being a top billboard song or not.
```{r eval=F}
log_model_list$year1960
```

#### How Logistic Regression Feature works in the app
From the initially selected tracks, one can select tracks that they want to plot against time. The `get_probability_of_billboard() function takes in the spotify audio features of a track and runs it against the logistic regressions for each year to output the probability of a song being on the billboard per year. 

Below is a sample of the dataframe of probabilities that feed into the dataframe for the plot.
```{r eval=F, include=F}
format_new_songs_logit <- function(songs){
  new_songs <- billboard::spotify_track_data[nrow(songs),]
  new_songs <- ""
  new_songs$artist_name <- songs$artist_name
  new_songs$track_name <- songs$track_uri #in the app this is track_name
  new_songs$duration_ms <- songs$duration_ms
  new_songs$danceability <- songs$danceability
  new_songs$energy <- songs$energy
  new_songs$key <- case_when(
    songs$key=="C"~0,
    songs$key=="C#"~1,
    songs$key=="Db"~1,
    songs$key=="D"~2,
    songs$key=="D#"~3,
    songs$key=="Eb"~3,
    songs$key=="E"~4,
    songs$key=="F"~5,
    songs$key=="F#"~6,
    songs$key=="Gb"~6,
    songs$key=="G"~7,
    songs$key=="G#"~8,
    songs$key=="Ab"~8,
    songs$key=="A"~9,
    songs$key=="A#"~10,
    songs$key=="Bb"~10,
    songs$key=="B"~11,
    TRUE~-1)
  new_songs$loudness <- songs$loudness
  new_songs$mode <- ifelse(songs$mode=="Major",1,0)
  new_songs$speechiness<- songs$speechiness
  new_songs$acousticness <- songs$acousticness
  new_songs$instrumentalness <- songs$instrumentalness
  new_songs$liveness <- songs$liveness
  new_songs$valence <- songs$valence
  new_songs$tempo <- songs$tempo
  new_songs$year <-  substr(songs$release_date, 1, 4)
  return(as.data.frame(new_songs))
}

```


```{r}
library(musictasteR)
input_song <- format_new_songs_logit(spotify_test_pull_with_characteristics[3,])
probability_df <- get_probability_of_billboard(input_song, log_model_list)
probability_df['true_song_year_bool'] <- probability_df$true_song_year == probability_df$year_int
head(probability_df)
```

The following plot shows a line graph of the probabilities that the song will be on the billboard throughout time. There are three points that are highlighted in the line graph, the probability of the release year, the highest probability and the lowest probability. 

```{r fig.width=8, fig.height=6}
plot_probabilities(probability_df, 3,2,4, 5)
```
Because there are only logistic regression models until 2015, if a song has a release year after 2016, the release year will not be highlighted in the app.

### Match: Clustering

#### Description

A clustering was done on the billboard top songs on a yearly basis. These models were created for each year separately to identify potential similar groupings with respect to the track features. 

Three methods were used for clustering to determine which one had maximum stability-
1) K Means Clustering
2) HCPC - Hierarchical Clustering on Principal Components
3) HCPC with Pre PCA - Hierarchical Clustering on Principal Components precluded by reduction into 2 dimensions using a PCA

Clustering through K-Means Method -

```{r eval=F}
cluster_year_k <- function(objet,year_taken,clust_size){
  
  # Restricting the input object to the input year
  filter_obj <- na.omit(objet) %>% filter(year==(year_taken))
  
  # Keeping only the numerical columns that will be useful for the clustering
  restr_obj <- filter_obj %>% select(danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, duration_ms, time_signature, year)
  
  # K means clustering
  clust_features <- kmeans(restr_obj,centers=clust_size,iter.max = 1000, nstart = 10)
  
  # Append into original input database and returning
  filter_obj$k_cluster <- paste0(year_taken,"_",clust_features$cluster)
  return(list(filter_obj,clust_features))
}
```

Clustering through HCPC Method -

```{r eval=F}
cluster_year_hcpc <- function(objet,year_taken,clust_num){
  
  # Restricting the input object to the input year
  filter_obj <- na.omit(objet) %>% filter(year==(year_taken))
  
  # Keeping only the numerical columns that will be useful for the clustering
  restr_obj <- filter_obj %>% select(danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, duration_ms, time_signature, year)
  
  # HCPC clustering
  clust_features <- HCPC(restr_obj, nb.clust = clust_num, graph = F)
  
  # Append into original input database and returning
  filter_obj$hcpc_cluster <- paste0(year_taken,"_",clust_features$data.clust$clust)
  return(list(filter_obj,clust_features))
}
```


Clustering through HCPC precluded with PCA (2 principal dimensions) -

```{r eval=F}

cluster_year_hcpc_pca <- function(objet,year_taken,clust_num){
  
  # Restricting the input object to the input year
  filter_obj <- na.omit(objet) %>% filter(year==(year_taken))
  
  # Keeping only the numerical columns that will be useful for the clustering
  restr_obj <- filter_obj %>% select(danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, duration_ms, time_signature, year)
  
  # Running the PCA for preprocessing the input database
  pca_res <- PCA(restr_obj,ncp=2,graph = F)
  
  # Clustering with HCPC based on the PCA input
  clust_features <- HCPC(pca_res, nb.clust = clust_num, graph = F)
  
  # Adding Principal dimensions and Clusters to the database and returning
  filter_obj$dim_1 <- pca_res$ind$coord[,1]
  filter_obj$dim_2 <- pca_res$ind$coord[,2]
  filter_obj$hcpc_pca_cluster <- paste0(year_taken, "_", clust_features$data.clust$clust)
  return(list(filter_obj,clust_features))
}

```

#### Compilation functions

The clustering functions were applied to each year to create a list of model objects, so as to predict the cluster of a song based on its features for any given year. This is for all of the three models shown in the previous section.

These clusters was also created on the billboard dataframe.

K means clusters database -
```{r eval=F}

# Setting range for the processing yearly
min_year <- min(features_sample$year)
max_year <- max(features_sample$year)

# Creating empty dataframe/list for appending
features_clustered_k <- data.frame()
model_k_clusters <- list()

# Creating a model and clusters for each year through a loop
for (year in min_year:max_year) {
    temp <- cluster_year_k(features_sample_proc,year,3)
    features_clustered_k <- rbind(features_clustered_k,as.data.frame(temp[1]))
    model_k_clusters <- append(model_k_clusters,temp[2])
}
```

HCPC clusters database-
```{r eval=F}
features_clustered_hcpc <- data.frame()
model_hcpc_clusters <- list()
    
# Creating a model and clusters for each year through a loop
for (year in min_year:max_year) {
    temp <- cluster_year_hcpc(features_sample_proc,year,3)
    features_clustered_hcpc <- rbind(features_clustered_hcpc,as.data.frame(temp[1]))
    model_hcpc_clusters <- append(model_hcpc_clusters,temp[2])
}
```

HCPC with pre PCA clusters database-
```{r eval=F}
features_clustered_hcpc_pca <- data.frame()
model_hcpc_pca_clusters <- list()
    
# Creating a model and clusters for each year through a loop
for (year in min_year:max_year) {
    temp <- cluster_year_hcpc_pca(features_sample_proc,year,3)
    features_clustered_hcpc_pca <- rbind(features_clustered_hcpc_pca,as.data.frame(temp[1]))
    model_hcpc_pca_clusters <- append(model_hcpc_pca_clusters,temp[2])
}
```

The databases with individual clusters were combined into one containing all three.

Combination of Cluster databases
```{r eval=F}
# Creating a combined database with all the three clustering types
combined_clusters <- features_clustered_hcpc_pca
combined_clusters$hcpc_cluster <- features_clustered_hcpc$hcpc_cluster
combined_clusters$k_cluster <- features_clustered_k$k_cluster
```

The "combined_clusters" data frame is the output database

#### Create PCA linear models

The prediction of clusters and plotting on principal dimensions requires each newly added song to have its principal dimensions in the dataframe. This has been done for each individual song through a linear model, predicting the principal dimensions using the quantitative variables, on each year separately. The output of this is 2 models, one each for the first two principal dimensions of the track, for each year, stored as a list of models.

Function to create the linear models for PCA given a year -
```{r eval=F}

pc_as_lm <- function(objet,year_taken){
  
  # Only using the year inputed
  filter_obj <- objet %>% filter(year==(year_taken))
  
  # Creating linear models 1 and 2 for the two principal dimensions using the numerical features
  mod1 <- lm(dim_1~danceability+energy+key+loudness+mode+speechiness+acousticness+instrumentalness+liveness+valence+tempo+duration_ms,data=filter_obj)
  mod2 <- lm(dim_2~dim_1+danceability+energy+key+loudness+mode+speechiness+acousticness+instrumentalness+liveness+valence+tempo+duration_ms,data=filter_obj)
  
  # Return the year along with the two models as a list
  return(list(year_taken,mod1,mod2))
}

```

Run loop on a yearly basis to create list of linear models for first two principal dimensions-
```{r eval=F}
bb_pc_lm_dim1 <- list()
bb_pc_lm_dim2 <- list()
    
# Running the model on a yearly basis to pull in the linear models for each
for (years in min_year:max_year) {
    temp <- pc_as_lm(bb_combined_clusters,years)
    bb_pc_lm_dim1 <- append(bb_pc_lm_dim1,temp[2])
    bb_pc_lm_dim2 <- append(bb_pc_lm_dim2,temp[3])
}
```


