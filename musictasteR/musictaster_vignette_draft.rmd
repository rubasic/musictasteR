---
title: "Getting the data from spotify"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{dplyr compatibility}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

This vignette describes the MusicTasteR package, the process from its conception to its execution, and its final capabilities and use cases.

```{r}
library(gridExtra)
library(spotifyr)
library(dplyr)
library(tidyverse)
library(httr)
library(purrr)
```

The user needs to first go on spotify to get their Client ID:
https://developer.spotify.com/documentation/web-api/

##Data Gathering

We needed a sample of average and top chart songs that we could analyse and see if there were different characteristics between the two song types. We explored various sources of music data on the net, including MusicBrainz (https://musicbrainz.org), Discogs api (https://www.discogs.com/developers/), Echo Nest (http://the.echonest.com/) but eventually settled on Spotify due to their lenient API rate usage policies, and the simplicity of the music attribute outputs. 

For the top charts data, we found the following package: https://cran.r-project.org/web/packages/billboard/index.html, which gave us a dataset of about 90-100 top charts songs froom 1960 to 2015. It already had the spotify uri and the audio features included in the dataframe, which was very convenient for us to use.

In order to build a database of "average" songs, we searched on the internet for a flat file of a sample of average songs. We came across the One Million Song Database (https://labrosa.ee.columbia.edu/millionsong/) which we tried to use as our sample of "average songs." However, we ran into two problems with this dataset. The first was that using track name and artist name to search for the spotify URL did not always yield accurate matches, as song titles and artist names could be different across sources. Another problem was that we only had data from the 1980s onwards with most of the data coming from the 2010s, which was not the distribution of the billboard data we had. Therefore, we decided to supplement the years we were missing with data sampled directly from spotify.

Spotify api search track function that had a wildcard feature, and it also provided a filter on the album year. We decided to leverage these features to sample random songs directly from spotify. 

Below is the function that we used to match the songs from the one million song database to the spotify data.
```{r eval=F}
get_track_id <- function(row) {
  track_name = gsub(' ','%20', gsub("[^[:alnum:][:space:]]",'',row['song']))
  artist_name =gsub(' ','%20', gsub("[^[:alnum:][:space:]]",'',row['artist']))
  api <- str_glue('https://api.spotify.com/v1/search/?q=track:{track_name}%20artist:{artist_name}&type=track&limit=1')
  access_token <- get_spotify_access_token()
  result <- RETRY('GET', url = api, query = list(access_token = <INSERT_ACCESS_TOKEN>), quiet = TRUE, times = 1, pause_min = 1) %>% content 
    tryCatch({
      final_id <- suppressWarnings(result$tracks$items[[1]]$id)
      return (final_id)
    }, error=function(error_cond) {
      if (error_cond$message == 'subscript out of bounds') {
        return('ERROR subscript')
      }
    }
    , error=function(error_cond) {
      if (error_cond$message =='$ operator is invalid for atomic vectors') {
        result2 <- RETRY('GET', url = api, query = list(access_token = <INSERT_ACCESS_TOKEN>), quiet = TRUE, times = 1, pause_min = 1) %>% content 
        print(error_cond$message)
        print(api)
        return(result2$tracks$items[[1]]$id)
      }
      else {
        print(error_cond$message)
        return('ERROR mysterious')
      }
    }
  )
}
```

### Sampling the Data

AKSHAY


Script that searches by year based on top words in song titles
```{r eval=F}
all_tracks_upd <- list()
for (i in 1:98) {
  for (year in 1960:2019) {
    
    # Top words is a list of top 100 songs, link given in the description
    track_search <-  paste0('*',top_words[i,],'*')
    
    #URL to query
    url_built <- str_glue('https://api.spotify.com/v1/search?q=track:{track_search}%20year:{year}&limit=50&offset=60&type=track')
      res <- RETRY('GET', url = url_built, query = list(access_token = access_token), quiet = TRUE, times = 1, pause_min = 100) %>% content
    if((!is.null(names(res)))){
    res <- res %>% .$tracks %>% .$items
    all_tracks_upd <- append(all_tracks_upd,res)
    }
  }
  # Testing to see if the word was correctly used
  print(top_words[i,])
}

```

```{r eval=F}
#transforms list of results into a dataframe
tracks_df_upd <- map_df(seq_len(length(all_tracks_upd)), function(x) {
                list(
                    track_name = all_tracks_upd[[x]]$name,
                    track_uri = gsub('spotify:track:', '', all_tracks_upd[[x]]$uri),
                    artist_name = all_tracks_upd[[x]]$artists[[1]]$name,
                    artist_uri = all_tracks_upd[[x]]$artists[[1]]$id,
                    album_name = all_tracks_upd[[x]]$album$name,
                    album_id = all_tracks_upd[[x]]$album$id,
                    album_year= all_tracks_upd[[x]]$album$release_date
                )
            })

# Pulling the last 4 digits in the year, because some songs contain the full date of release
sample_by_year_upd <- tracks_df_upd %>% mutate(album_year_4dgt=substr(album_year,1,4) ) 
```

Test to see if restriction at 7 songs per artist per year will cut down the total songs by a considerable amount -
```{r eval=F}
artist_freq <- sample_by_year_upd %>%
  group_by(artist_name,album_year_4dgt) %>%
  summarise(freq=n()) 

artist_freq$recalc_songs=ifelse(artist_freq$freq>7,7,artist_freq$freq)

#Total songs
artist_freq %>%
  ungroup() %>%
  summarise(tots=sum(recalc_songs))


ggplot(data=artist_freq, aes(x=freq)) + geom_histogram(bins=30) 
```


Restricting to 7 songs per artist per year, to prevent over-sampling from one artist. Selection of songs will be random -
```{r eval=F}
sample_songs_edit <- sampled_songs_worded %>%
  select(-album_year) %>%
  distinct() %>%
  arrange(album_year_4dgt,artist_name) %>%
  group_by(album_year_4dgt,artist_name) %>%
  slice(1:7) %>%
  ungroup()

# Test to see distribution
sample_songs_edit %>% distinct(artist_name) %>% summarise(total_artists=n())

sample_songs_edit %>% group_by(album_year_4dgt) %>% summarise(count=n()) %>% ggplot(aes(x=album_year_4dgt,y=count)) + geom_bar(stat='identity')
```

Applying a Max cap at 3000 songs per year -
```{r eval=F}
sample_songs_edit <- sample_songs_edit %>%
  arrange(album_year_4dgt,artist_name)

#First keep as many unique artists as possible
sample_songs_edit$count=1

# Setting a counter for each song by an artist in a year
for(i in 2:nrow(sample_songs_edit)){
  if(sample_songs_edit$artist_name[i]==sample_songs_edit$artist_name[i-1]&sample_songs_edit$album_year_4dgt[i]==sample_songs_edit$album_year_4dgt[i-1]){
    sample_songs_edit$count[i]=sample_songs_edit$count[i-1]+1
  }
}

# Selecting on the basis of the counter variable, so that 1 song from each artist is selected and then 2nd from each artist and so on
sample_songs_edit <- sample_songs_edit %>%
  arrange(album_year_4dgt,artist_name,count) %>%
  group_by(album_year_4dgt) %>%
  slice(1:3000) %>%
  ungroup()

sample_songs_edit %>% distinct(artist_name) %>% summarise(total_artists=n())

sample_songs_edit %>% group_by(album_year_4dgt) %>% summarise(count=n()) %>% ggplot(aes(x=album_year_4dgt,y=count)) + geom_bar(stat='identity')
```

Sample_songs_edit is the final pre-processed dataframe used

The following functions were used to grab the audio features from spotify:
```{r}
make_api_call <- function(audio_features_api,access_token) {
  analysis_result_call <- safely(RETRY)('GET', url = audio_features_api, query = list(access_token = access_token), quiet = TRUE, times = 1, pause_min = 100) 
  if (is.null(analysis_result_call$result)) {
    print("ERROR in call for audio features")
    print(analysis_result_call$error)
  } else {
  analysis_result <- analysis_result_call$result %>% content
  track_href <- analysis_result$track_href
  track_result_call <- safely(RETRY)('GET', url = track_href, query = list(access_token = access_token), quiet = TRUE, times = 1, pause_min = 100)
  analysis_result <- analysis_result[c('danceability',
                                       'energy',
                                       'key',
                                       'loudness',
                                       'mode',
                                       'speechiness',
                                       'acousticness',
                                       'instrumentalness',
                                       'liveness',
                                       'valence',
                                       'tempo',
                                       'type',
                                       'uri',
                                       'track_href',
                                       'analysis_url',
                                       'duration_ms',
                                       'time_signature')]
    track_basic_trait_result <- track_result_call$result %>% content
    analysis_result['artist_name'] <- track_basic_trait_result$artists[[1]]$name
    analysis_result['track_name'] <- track_basic_trait_result$name
    analysis_result['year'] <- track_basic_trait_result$album$release_date
  return(analysis_result)
  }
}

get_audio_features <- function(row) {
    track_uri <- as.character(row["track_uri"])
    audio_features_api <- str_glue('https://api.spotify.com/v1/audio-features/{track_uri}')
    access_token <- get_spotify_access_token()
    analysis_result <- safely(make_api_call)(audio_features_api,access_token)
    if (is.null(analysis_result$error)) {
      return(analysis_result)
    } else {
      print("ERROR in overall call")
      print(analysis_result$error)
    }
}


head_of_df <- df
final_list <- list()
fail_list <- list()

#loop that runs the api fetch of musi
for (i in 1:nrow(head_of_df)) {
  if (i %% 20 == 0) {
    print(i)
    #randomly sleep for a little bit   
    Sys.sleep(sample(seq(0.5, 2.5, 0.5), 1))
  }
  row <- unlist(head_of_df[i,])
  api_result <- get_audio_features(row)
  #error hndling
  if (is.null(api_result$result)) {
    #wrong url
    if (api_result$message == "Must specify at least one of url or handle") {
      print(paste("Error in id", i))
      # return("URL misinput error")
    } else {
      print(paste('Other kind of error in id', i))
      print(api_result$message)
    }
    ## put other error handling here
  } else {
    final_row <- c(unlist(row[c("track_name", "track_uri", "album_name", "album_year", "album_year_4dgt", "artist_name", "artist_uri")]), unlist(api_result$result))
    final_list[[i]] <- final_row
  }
}

#filter out the null entries in the list
final_list_filtered <- Filter(Negate(is.null), final_list)

#make the list into a dataframe
massive <- map_df(seq_len(length(final_list_filtered)), function(x) {
                list(
                  track_name = final_list_filtered[[x]]["track_name"],
                  track_uri = final_list_filtered[[x]]["track_uri"],
                  album_name = final_list_filtered[[x]]["album_name"],
                  album_year = final_list_filtered[[x]]["album_year"],
                  album_year_4dgt = final_list_filtered[[x]]["album_year_4dgt"],
                  danceability = final_list_filtered[[x]]["danceability"],
                  energy = final_list_filtered[[x]]["energy"],
                  key = final_list_filtered[[x]]["key"],
                  loudness = final_list_filtered[[x]]["loudness"],
                  mode = final_list_filtered[[x]]["mode"],
                  speechiness = final_list_filtered[[x]]["speechiness"],
                  acousticness = final_list_filtered[[x]]["acousticness"],
                  instrumentalness = final_list_filtered[[x]]["instrumentalness"],
                  liveness = final_list_filtered[[x]]["liveness"],
                  valence = final_list_filtered[[x]]["valence"],
                  tempo = final_list_filtered[[x]]["tempo"],
                  type = final_list_filtered[[x]]["type"],
                  uri = final_list_filtered[[x]]["uri"],
                  track_href = final_list_filtered[[x]]["track_href"],
                  analysis_url = final_list_filtered[[x]]["analysis_url"],
                  duration_ms = final_list_filtered[[x]]["duration_ms"],
                  time_signature = final_list_filtered[[x]]["time_signature"],
                  artist_name = final_list_filtered[[x]]["artist_name"],
                  artist_id = final_list_filtered[[x]]["artist_uri"]
                )})
```


## Logistic Regression

One of the features of the app is to predict how successful a song would perform. For this, we implemented a logistic regression to find the probability of whether a song would be in the top charts or not. First, we needed to make sure that for every year, we had an equal proportion of non-billboard songs to billboard songs. Because the billboard songs quantity was relatively fixed across the years, we reduced the number of average songs in our sample per year. Variable selection for the logistic regression was performed using the stepwise method that used both the forward and backward procedure optimizing on the BIC criterion. We calibrated one model per year and saved it into a list of models, along with their fisher test scores to check global signficance of the model. The models were saved as a .rda object and is accessible in the package as "log_model_list."

```{r}
#load the large data
df <- averagesongs
df <- df[!is.na(df$key),]
billboard_df <- billboard::spotify_track_data

#prepare large dataframes, align columns and combine the datasets
#this is the average songs data
df_edit <- df[c("track_uri","year","track_name", "artist_name","duration_ms", "danceability","energy","key","loudness","mode","speechiness","acousticness","instrumentalness","liveness","valence","tempo")]
colnames(df_edit)[1] <- "track_id"
df_edit["top_or_not"] <- 0

#this is the billboard data
billboard_edit <- billboard_df[c("track_id","year","track_name", "artist_name","duration_ms", "danceability","energy","key","loudness","mode","speechiness","acousticness","instrumentalness","liveness","valence","tempo")]
billboard_edit["top_or_not"] <- 1
full_test <- rbind(billboard_edit,df_edit)
full_test <- full_test[!(duplicated(full_test$track_name)& duplicated(full_test$artist_name)),]

#filter out artists that show up more than 3 times
artist_test <- full_test %>% filter(top_or_not == 0) %>% group_by(year, artist_name) %>% slice(1:3)
artist_test%>% group_by(year, artist_name) %>% summarise(count=n())
artist_test <- artist_test %>% group_by(year) %>% slice(1:600) %>% ungroup() #limit each year to only have 600 average songs
artist_test %>% group_by(year) %>% summarise(count=n()) 
new_artist_test <-rbind(billboard_edit, artist_test) #finally, combine the average and the billboard songs
per_year_test <- new_artist_test %>% group_by(year) %>% slice(1:600)
```

```{r fig.width=14}
#Original Distribution of songs per year
full_test %>% filter(!year%in% c(2015:2019)) %>% group_by(year, top_or_not) %>% summarise(count=n()) %>% ggplot(aes(x=year, y=count, fill=as.factor(top_or_not))) + geom_bar(stat="identity") + labs(title="Original Distribution of Average to Top Chart Songs")

#Look at the distribution of songs per year
new_artist_test %>% filter((!year%in% c(2015:2019))) %>% group_by(year, top_or_not) %>% summarise(count=n()) %>% ggplot(aes(x=year, y=count, fill=as.factor(top_or_not))) + geom_bar(stat="identity") + labs(title="Filtered Distribution of Average to Top Chart Songs")

#define function that runs logistic regression for one year
run_l_reg <-function(full_test) {
  res <- glm(top_or_not~.,family=binomial(link='logit'), data=full_test[-c(1:4)])
  bic <- step(res,direction="both",k=3,trace = FALSE)
  res2 <- glm(bic$model,family=binomial(link='logit'),data=full_test[-c(1:4)])
  anova_res <- anova(update(res2, ~1), res2, test='LR')
  return(list(bic, anova_res$`Pr(>Chi)`[2]))
}

#run the logistic regression for every year
get_all_reg_models <- function(full_test) {
  all_results <- list()
  anova_list <- list()
  return_list <- list()
  for (year_input in 1960:2015) {
    full_test_year <- full_test %>% filter(year == year_input)
    result <- run_l_reg(full_test_year)
    all_results[paste0('year', as.character(year_input))] <- result[1]
    anova_list[paste0('year', as.character(year_input))] <- result[2]
  }
  return_list[['model_list']] <- all_results
  return_list[['anova_list']] <- anova_list
  return(return_list)
}

clean_up <- function(input) {
                list(
                  danceability = input$result$danceability,
                  energy = input$result$energy,
                  key = input$result$key,
                  loudness = input$result$loudness,
                  mode = input$result$mode,
                  speechiness = input$result$speechiness,
                  acousticness = input$result$acousticness,
                  instrumentalness = input$result$instrumentalness,
                  liveness = input$result$liveness,
                  valence = input$result$valence,
                  tempo = input$result$tempo,
                  duration_ms =  input$result$duration_ms,
                  # uri = final_list_filtered[[x]]["uri"],
                  # track_href = final_list_filtered[[x]]["track_href"],
                  # analysis_url = final_list_filtered[[x]]["analysis_url"],
                  # duration_ms = final_list_filtered[[x]]["duration_ms"],
                  # time_signature = final_list_filtered[[x]]["time_signature"],
                  artist_name = input$result$artist_name,
                  track_name = input$result$track_name,
                  year = input$result$year
                  # artist_id = final_list_filtered[[x]]["artist_uri"]
                )
  }

get_all_reg_models(new_artist_test)

save_result <- get_all_reg_models(new_artist_test)

# saveRDS(save_result$model_list, file="all_models2.rda")

```

All models had significant liklihood ratio test scores. 1972 has the highest Liklihood ratio test score of 2.86441e-09, meaning it is the least significant model. The models are saved in the package and can be accessed through an object called "log_model_list"
```{r}
anova_list <- unlist(save_result$anova_list)
anova_df <- data.frame(anova_score = unlist(anova_list))
anova_df['year'] <- sapply(rownames(anova_df), function(x) {return(substring(x,5,9))})

# All anova scores
a <- ggplot() + geom_point(data=anova_df,aes(x=year, y=anova_score))+labs(main="Anova score for models")
b <- anova_df %>% filter(year != 1972) %>% ggplot()+ geom_point(aes(x=year, y=anova_score))+labs(main="Anova scores for models except 1972")
grid.arrange(a,b, ncol=2)
```

Below is a sample output of one of the models (1960 model) in the model list object. For every year, we can see the variables that were relevant in the prediction of being a top billboard song or not.
```{r}
log_model_list$year1960
```

## How Logistic Regression Feature works in the app
From the initially selected tracks, one can select tracks that they want to plot against time. The `get_probability_of_billboard() function takes in the spotify audio features of a track and runs it against the logistic regressions for each year to output the probability of a song being on the billboard per year. 

Below is a sample of the dataframe of probabilities that feed into the dataframe for the plot.
```{r eval=F, include=F}
format_new_songs_logit <- function(songs){
  new_songs <- billboard::spotify_track_data[nrow(songs),]
  new_songs <- ""
  new_songs$artist_name <- songs$artist_name
  new_songs$track_name <- songs$track_artist_name
  new_songs$duration_ms <- songs$duration_ms
  new_songs$danceability <- songs$danceability
  new_songs$energy <- songs$energy
  new_songs$key <- case_when(
    songs$key=="C"~0,
    songs$key=="C#"~1,
    songs$key=="Db"~1,
    songs$key=="D"~2,
    songs$key=="D#"~3,
    songs$key=="Eb"~3,
    songs$key=="E"~4,
    songs$key=="F"~5,
    songs$key=="F#"~6,
    songs$key=="Gb"~6,
    songs$key=="G"~7,
    songs$key=="G#"~8,
    songs$key=="Ab"~8,
    songs$key=="A"~9,
    songs$key=="A#"~10,
    songs$key=="Bb"~10,
    songs$key=="B"~11,
    TRUE~-1)
  new_songs$loudness <- songs$loudness
  new_songs$mode <- ifelse(songs$mode=="Major",1,0)
  new_songs$speechiness<- songs$speechiness
  new_songs$acousticness <- songs$acousticness
  new_songs$instrumentalness <- songs$instrumentalness
  new_songs$liveness <- songs$liveness
  new_songs$valence <- songs$valence
  new_songs$tempo <- songs$tempo
  new_songs$year <-  substr(songs$release_date, 1, 4)
  return(as.data.frame(new_songs))
}

```

```{r}
input_song <- format_new_songs_logit(spotify_test_pull_with_characteristics[1,])

probability_df <- get_probability_of_billboard(input_song, log_model_list)
probability_df['true_song_year_bool'] <- probability_df$true_song_year == probability_df$year_int
head(probability_df)
```

The following plot shows a line graph of the probabilities that the song will be on the billboard throughout time. There are three points that are highlighted in the line graph, the probability of the release year, the highest probability and the lowest probability. 

```{r}
plot_probabilities(probability_df, 3,2,4, 5)
```
Because there are only logistic regression models until 2015, if a song has a release year after 2016, the release year will not be highlighted in the app.

# Clustering Functions -

Clustering through K-Means
```{r}
cluster_year_k <- function(objet,year_taken,clust_size){
  
  # Restricting the input object to the input year
  filter_obj <- na.omit(objet) %>% filter(year==(year_taken))
  
  # Keeping only the numerical columns that will be useful for the clustering
  restr_obj <- filter_obj %>% select(danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, duration_ms, time_signature, year)
  
  # K means clustering
  clust_features <- kmeans(restr_obj,centers=clust_size,iter.max = 1000, nstart = 10)
  
  # Append into original input database and returning
  filter_obj$k_cluster <- paste0(year_taken,"_",clust_features$cluster)
  return(list(filter_obj,clust_features))
}
```

Clustering through HCPC
```{r}
cluster_year_hcpc <- function(objet,year_taken,clust_num){
  
  # Restricting the input object to the input year
  filter_obj <- na.omit(objet) %>% filter(year==(year_taken))
  
  # Keeping only the numerical columns that will be useful for the clustering
  restr_obj <- filter_obj %>% select(danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, duration_ms, time_signature, year)
  
  # HCPC clustering
  clust_features <- HCPC(restr_obj, nb.clust = clust_num, graph = F)
  
  # Append into original input database and returning
  filter_obj$hcpc_cluster <- paste0(year_taken,"_",clust_features$data.clust$clust)
  return(list(filter_obj,clust_features))
}
```


Clustering through HCPC with pre PCA (2 principal dimensions)
```{r}

cluster_year_hcpc_pca <- function(objet,year_taken,clust_num){
  
  # Restricting the input object to the input year
  filter_obj <- na.omit(objet) %>% filter(year==(year_taken))
  
  # Keeping only the numerical columns that will be useful for the clustering
  restr_obj <- filter_obj %>% select(danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, duration_ms, time_signature, year)
  
  # Running the PCA for preprocessing the input database
  pca_res <- PCA(restr_obj,ncp=2,graph = F)
  
  # Clustering with HCPC based on the PCA input
  clust_features <- HCPC(pca_res, nb.clust = clust_num, graph = F)
  
  # Adding Principal dimensions and Clusters to the database and returning
  filter_obj$dim_1 <- pca_res$ind$coord[,1]
  filter_obj$dim_2 <- pca_res$ind$coord[,2]
  filter_obj$hcpc_pca_cluster <- paste0(year_taken, "_", clust_features$data.clust$clust)
  return(list(filter_obj,clust_features))
}

```

# Compilation functions

K means clusters database
```{r}

# Setting range for the processing yearly
min_year <- min(features_sample$year)
max_year <- max(features_sample$year)

# Creating empty dataframe/list for appending
features_clustered_k <- data.frame()
model_k_clusters <- list()

# Creating a model and clusters for each year through a loop
for (year in min_year:max_year) {
    temp <- cluster_year_k(features_sample_proc,year,3)
    features_clustered_k <- rbind(features_clustered_k,as.data.frame(temp[1]))
    model_k_clusters <- append(model_k_clusters,temp[2])
}
```

HCPC clusters database
```{r}
features_clustered_hcpc <- data.frame()
model_hcpc_clusters <- list()
    
# Creating a model and clusters for each year through a loop
for (year in min_year:max_year) {
    temp <- cluster_year_hcpc(features_sample_proc,year,3)
    features_clustered_hcpc <- rbind(features_clustered_hcpc,as.data.frame(temp[1]))
    model_hcpc_clusters <- append(model_hcpc_clusters,temp[2])
}
```

HCPC with pre PCA clusters database
```{r}
features_clustered_hcpc_pca <- data.frame()
model_hcpc_pca_clusters <- list()
    
# Creating a model and clusters for each year through a loop
for (year in min_year:max_year) {
    temp <- cluster_year_hcpc_pca(features_sample_proc,year,3)
    features_clustered_hcpc_pca <- rbind(features_clustered_hcpc_pca,as.data.frame(temp[1]))
    model_hcpc_pca_clusters <- append(model_hcpc_pca_clusters,temp[2])
}
```

Combination of Cluster databases
```{r}
# Creating a combined database with all the three clustering types
combined_clusters <- features_clustered_hcpc_pca
combined_clusters$hcpc_cluster <- features_clustered_hcpc$hcpc_cluster
combined_clusters$k_cluster <- features_clustered_k$k_cluster
```

"combined_clusters" is the output database



# Create PCA linear models -

PCA components as linear components of Variables - Yearwise
```{r}

pc_as_lm <- function(objet,year_taken){
  
  # Only using the year inputed
  filter_obj <- objet %>% filter(year==(year_taken))
  
  # Creating linear models 1 and 2 for the two principal dimensions using the numerical features
  mod1 <- lm(dim_1~danceability+energy+key+loudness+mode+speechiness+acousticness+instrumentalness+liveness+valence+tempo+duration_ms,data=filter_obj)
  mod2 <- lm(dim_2~dim_1+danceability+energy+key+loudness+mode+speechiness+acousticness+instrumentalness+liveness+valence+tempo+duration_ms,data=filter_obj)
  
  # Return the year along with the two models as a list
  return(list(year_taken,mod1,mod2))
}

```

Creating Yearly model
```{r}
bb_pc_lm_dim1 <- list()
bb_pc_lm_dim2 <- list()
    
# Running the model on a yearly basis to pull in the linear models for each
for (years in min_year:max_year) {
    temp <- pc_as_lm(bb_combined_clusters,years)
    bb_pc_lm_dim1 <- append(bb_pc_lm_dim1,temp[2])
    bb_pc_lm_dim2 <- append(bb_pc_lm_dim2,temp[3])
}
```

"bb_pc_lm_dim1" and "bb_pc_lm_dim2" are the R objects for the list of linear models on a yearly basis


